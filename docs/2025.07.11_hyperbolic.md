## This Week's Progress (hyperbolic embeddings)

### **TODO**
- [x] investigate hyperbolic embedding implementation
- [x] list up tasks
- [x] Survey papers to read
- [x] Survey websites about PoincarÃ© Embedding
- [x] Planning the implementation of Poincare Embedding
- [x] Try to use Poincare Embeddings implemented by Python

---

### **Papers of Poincare Embedding**
- PoincarÃ© Embeddings for
Learning Hierarchical Representations  
https://arxiv.org/pdf/1705.08039.   
This paper is the original source, so I need to understand all of it.    
detail:[Poincareï¼¿Embedding](/docs/Poincare_Embedding.md)

- Tried to implement Poincare Embedding.   
http://www.tani.cs.chs.nihon-u.ac.jp/g-2018/ML_PE/poincare.pdf.   
Although they haven't reached the implementation stage, it looks like it will be a good aid for understanding the theory.

---
### **survey websites about PoincarÃ© Embdding**
- ç•°ç©ºé–“ã¸ã®åŸ‹ã‚è¾¼ã¿ï¼Poincare EmbeddingsãŒæ‹“ãè¡¨ç¾å­¦ç¿’ã®æ–°å±•é–‹ï¼š    
https://tech-blog.abeja.asia/entry/poincare-embeddings.   
Is the section I should implement this time "link prediction in graphs"?    
It seems I'll need to read this part of the paper carefully.  
ğŸŒŸLearning is more effective when data with hierarchical or graph structures is embedded into a Riemannian manifold called hyperbolic space.         
ğŸŒŸI want to create an embedding where the distance is short if an edge exists between nodes (i.e., a parent-child relationship is established), and the distance is long if no edge exists.          
ğŸŒŸIt can also quantify things like the semantic similarity between words

---
### **Planning the implementation of PoincarÃ© Embedding**
â†“ PoincarÃ© Embedding is implemented in gensim (Python).       
https://github.com/piskvorky/gensim/blob/develop/gensim/models/poincare.py

While other implementations exist, the code above seems easy to follow because it is well-organized.
 
List of links to potentially helpful code references:        
https://github.com/facebookresearch/poincare-embeddings.     
https://github.com/HazyResearch/hyperbolics.    

---
### **Try to use Poincare Embeddings implemented by Python**
https://github.com/piskvorky/gensim/blob/develop/gensim/models/poincare.py

```bash
pip3 install gensim
```

```python

# needs $ pip3 install gensim
from gensim.models.poincare import PoincareModel, PoincareRelations
from gensim.test.utils import datapath

# 1. prepare data
file_path = datapath('/Users/honokakobayashi/dev/Univ/Study/Poincare_test/tree.edges')
relations = PoincareRelations(file_path)

"""
relations = [
    ('ã‚«ãƒ³ã‚¬ãƒ«ãƒ¼', 'æœ‰è¢‹é¡'),
    ('ã‚³ã‚¢ãƒ©', 'æœ‰è¢‹é¡'),
    ('æœ‰è¢‹é¡', 'å“ºä¹³é¡'),
    ('ã‚¤ãƒŒ', 'å“ºä¹³é¡')
]
"""

# 2. initialize and train the model
model = PoincareModel(relations, size=5, negative=2)
model.train(epochs=50)
print("---------- Embeddings ----------")
print(model.kv.vectors)

# 3. check the results
print("---------- results ----------")
print("most similar words of æ†ã‚€:")
print(model.kv.most_similar('æ†ã‚€', topn=5))
print("most similar words of æ•‘ã„:")
print(model.kv.most_similar('æ•‘ã„', topn=5))
print("most similar words of è¨€è‘‰:")
print(model.kv.most_similar('è¨€è‘‰', topn=5))
print("most similar words of å®¶æ—:")
print(model.kv.most_similar('å®¶æ—', topn=5))
print("most similar words of å¥½ã:")
print(model.kv.most_similar('å¥½ã', topn=5))

print("distance between 'å¥½ã' and 'æ†ã‚€':", model.kv.distance('å¥½ã', 'æ†ã‚€'))
```

**result**
```
---------- Embeddings ----------
[[-0.72857252  0.24202885 -0.31484642  0.07590013 -0.01453852]
 [-0.66070417  0.23234398 -0.29315745  0.07594059 -0.02807687]
 [ 0.53697957  0.5195589  -0.32299774  0.01099925  0.01572869]
 ...
 [ 0.17085903 -0.20927281  0.20851365 -0.21170045 -0.2401501 ]
 [ 0.14538881 -0.20067557  0.17587388 -0.18721296 -0.22489457]
 [ 0.16827687 -0.22848292  0.217612   -0.22975941 -0.2594983 ]]
---------- results ----------
most similar words of æ†ã‚€:
[('å«ŒãŒã‚‹', 0.04288574847897562), ('å¿Œã‚€', 0.05627940188863539), ('æ…¢ä¾®', 0.05722351430671617), ('è»½è”‘', 0.0579676669899697), ('è¦‹ç¸Šã‚‹', 0.06326857353072379)]
most similar words of æ•‘ã„:
[('æ‰¶ã‘ã‚‹', 0.036388464212531293), ('ä¼ä¾', 0.038001800579493136), ('æ‰¶ç¿¼', 0.04476115580676383), ('åŠ›ã‚’è²¸ã™', 0.04816697045796615), ('åŠ›æ·»ãˆ', 0.048930536733188916)]
most similar words of è¨€è‘‰:
[('è­°è«–', 0.058520123285874415), ('ã²ã¨ã‚Šè¨€', 0.060958602091809894), ('ã¾ã˜ãªã„', 0.06168666130388902), ('ç§‘ç™½', 0.06218728162360296), ('å£ä¸Š', 0.06271614335460053)]
most similar words of å®¶æ—:
[('é¤Šå®¶', 1.6059021632104682), ('ç•ª', 1.6201238018232798), ('ç•ªã„', 1.655102550352118), ('å§»å®¶', 1.6605814128527356), ('ã‚¢ãƒ´ã‚§ãƒƒã‚¯', 1.6698396209324178)]
most similar words of å¥½ã:
[('åæ„›', 1.745744974929887), ('å¥½ãå¥½ã¿', 1.8888289586487232), ('å¥½ãã“ã®ã¿', 1.9035390038926814), ('æ„Ÿæ­', 1.927064436419263), ('åå¥½', 1.952441378034425)]
distance between 'å¥½ã' and 'æ†ã‚€': 3.6544957689673874
```

**Right now, there are about 3,000 input nodes(Hyper-Hypo Pairs). Even with that, I was able to get a plausible output, so it seems I can just follow this for this implementation.**
