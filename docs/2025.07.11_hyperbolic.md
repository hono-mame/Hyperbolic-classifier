## This Week's Progress (hyperbolic embeddings)

### **TODO**
- [x] investigate hyperbolic embedding implementation
- [x] list up tasks
- [x] Survey papers to read
- [x] Survey websites about Poincaré Embedding
- [x] Planning the implementation of Poincare Embedding
- [x] Try to use Poincare Embeddings implemented by Python

---

### **Papers of Poincare Embedding**
- Poincaré Embeddings for
Learning Hierarchical Representations  
https://arxiv.org/pdf/1705.08039.   
This paper is the original source, so I need to understand all of it.    
detail:[Poincare＿Embedding](/docs/Poincare_Embedding.md)

- Tried to implement Poincare Embedding.   
http://www.tani.cs.chs.nihon-u.ac.jp/g-2018/ML_PE/poincare.pdf.   
Although they haven't reached the implementation stage, it looks like it will be a good aid for understanding the theory.

---
### **survey websites about Poincaré Embdding**
- 異空間への埋め込み！Poincare Embeddingsが拓く表現学習の新展開：    
https://tech-blog.abeja.asia/entry/poincare-embeddings.   
Is the section I should implement this time "link prediction in graphs"?    
It seems I'll need to read this part of the paper carefully.  
🌟Learning is more effective when data with hierarchical or graph structures is embedded into a Riemannian manifold called hyperbolic space.         
🌟I want to create an embedding where the distance is short if an edge exists between nodes (i.e., a parent-child relationship is established), and the distance is long if no edge exists.          
🌟It can also quantify things like the semantic similarity between words

---
### **Planning the implementation of Poincaré Embedding**
↓ Poincaré Embedding is implemented in gensim (Python).       
https://github.com/piskvorky/gensim/blob/develop/gensim/models/poincare.py

While other implementations exist, the code above seems easy to follow because it is well-organized.
 
List of links to potentially helpful code references:        
https://github.com/facebookresearch/poincare-embeddings.     
https://github.com/HazyResearch/hyperbolics.    

---
### **Try to use Poincare Embeddings implemented by Python**
https://github.com/piskvorky/gensim/blob/develop/gensim/models/poincare.py

```bash
pip3 install gensim
```

```python

# needs $ pip3 install gensim
from gensim.models.poincare import PoincareModel, PoincareRelations
from gensim.test.utils import datapath

# 1. prepare data
file_path = datapath('/Users/honokakobayashi/dev/Univ/Study/Poincare_test/tree.edges')
relations = PoincareRelations(file_path)

"""
relations = [
    ('カンガルー', '有袋類'),
    ('コアラ', '有袋類'),
    ('有袋類', '哺乳類'),
    ('イヌ', '哺乳類')
]
"""

# 2. initialize and train the model
model = PoincareModel(relations, size=5, negative=2)
model.train(epochs=50)
print("---------- Embeddings ----------")
print(model.kv.vectors)

# 3. check the results
print("---------- results ----------")
print("most similar words of 憎む:")
print(model.kv.most_similar('憎む', topn=5))
print("most similar words of 救い:")
print(model.kv.most_similar('救い', topn=5))
print("most similar words of 言葉:")
print(model.kv.most_similar('言葉', topn=5))
print("most similar words of 家族:")
print(model.kv.most_similar('家族', topn=5))
print("most similar words of 好き:")
print(model.kv.most_similar('好き', topn=5))

print("distance between '好き' and '憎む':", model.kv.distance('好き', '憎む'))
```

**result**
```
---------- Embeddings ----------
[[-0.72857252  0.24202885 -0.31484642  0.07590013 -0.01453852]
 [-0.66070417  0.23234398 -0.29315745  0.07594059 -0.02807687]
 [ 0.53697957  0.5195589  -0.32299774  0.01099925  0.01572869]
 ...
 [ 0.17085903 -0.20927281  0.20851365 -0.21170045 -0.2401501 ]
 [ 0.14538881 -0.20067557  0.17587388 -0.18721296 -0.22489457]
 [ 0.16827687 -0.22848292  0.217612   -0.22975941 -0.2594983 ]]
---------- results ----------
most similar words of 憎む:
[('嫌がる', 0.04288574847897562), ('忌む', 0.05627940188863539), ('慢侮', 0.05722351430671617), ('軽蔑', 0.0579676669899697), ('見縊る', 0.06326857353072379)]
most similar words of 救い:
[('扶ける', 0.036388464212531293), ('伏侍', 0.038001800579493136), ('扶翼', 0.04476115580676383), ('力を貸す', 0.04816697045796615), ('力添え', 0.048930536733188916)]
most similar words of 言葉:
[('議論', 0.058520123285874415), ('ひとり言', 0.060958602091809894), ('まじない', 0.06168666130388902), ('科白', 0.06218728162360296), ('口上', 0.06271614335460053)]
most similar words of 家族:
[('養家', 1.6059021632104682), ('番', 1.6201238018232798), ('番い', 1.655102550352118), ('姻家', 1.6605814128527356), ('アヴェック', 1.6698396209324178)]
most similar words of 好き:
[('偏愛', 1.745744974929887), ('好き好み', 1.8888289586487232), ('好きこのみ', 1.9035390038926814), ('感歎', 1.927064436419263), ('偏好', 1.952441378034425)]
distance between '好き' and '憎む': 3.6544957689673874
```

**Right now, there are about 3,000 input nodes(Hyper-Hypo Pairs). Even with that, I was able to get a plausible output, so it seems I can just follow this for this implementation.**
