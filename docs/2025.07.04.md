## This week's progress

### TODO
- [ ] debug MLP implementation
- [x] (install lightblue on my own PC)

#### debug MLP implementation
As of last week, it looked like the Loss and yPred were all NaN, so I output yPred at each epoch.
As a result, I found that it was not NaN from the beginning, but became NaN starting from epoch 2.
```haskell
Loading embeddings...
Loading training data...
trainx shape: [9,400]
Initializing model...
Training...
yPred: Tensor Float [9,1] [[ 0.5255   ],
                    [ 0.5385   ],
                    [ 0.5250   ],
                    [ 0.5272   ],
                    [ 0.5310   ],
                    [ 0.5418   ],
                    [ 0.5270   ],
                    [ 0.5374   ],
                    [ 0.5374   ]]
yPred: Tensor Float [9,1] [[NaN],
                    [NaN],
                    [NaN],
                    [NaN],
                    [NaN],
                    [NaN],
                    [NaN],
                    [NaN],
                    [NaN]]
yPred: Tensor Float [9,1] [[NaN],
                    [NaN],
                    [NaN],
                    [NaN],
                    [NaN],
                    [NaN],
                    [NaN],
                    [NaN],
                    [NaN]]
```

I replaced my custom MLP definition with an implementation **using hasktorch-tools** to check if this resolves the issue.
â†’ Still, it becomes NaN from epoch 2.

The cause does not seem to be the function implementation.  
The only possible reason might be **gradient explosion**??

I probably need to review the definitions of the **hidden layers** and so on. Further investigation is required.

```
Loading embeddings...
Loading training data...
Initializing model...
Training...
inputs shape in trainMLP: [6523,400]
targets shape in trainMLP: [6523,1]
Iter 1: Loss = 7.103969
Iter 2: Loss = NaN
Iter 3: Loss = NaN
Iter 4: Loss = NaN
Iter 5: Loss = NaN
Iter 6: Loss = NaN
Iter 7: Loss = NaN
Iter 8: Loss = NaN
Iter 9: Loss = NaN
```

There seems to be a technique called **gradient clipping**.